{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Fine-tuning pretrained Herbert model for task of Named Entity Detection\n",
    "Named Entity Detection is task similar to more popular Named Entity Recognizing.\n",
    "Detection means we are trying to find Named Entity occurences, but we don't need to distinguish between different Named Entity classes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Install pre-requirements\n",
    "We will use *huggingface* workflow with *pytorch* framework to train model and to operate sets we use ðŸ¤— *datasets*.\n",
    "We also use *sklearn* for metrics and *tensorboard* for logging training progress.\n",
    "To install *pytorch* the best way is to reference official installation page in *pytorch* website: https://pytorch.org/get-started/locally/.\n",
    "For ðŸ¤— transformers refer to: https://huggingface.co/docs/transformers/installation\n",
    "For ðŸ¤— datasets: https://huggingface.co/docs/datasets/installation\n",
    "For sklearn: https://scikit-learn.org/stable/install.html\n",
    "For tensorboard: https://pypi.org/project/tensorboard/ for pypi package or https://anaconda.org/conda-forge/tensorboard for conda.\n",
    "\n",
    "\n",
    "##### *Note*\n",
    "*If possible choose combination which will allow hardware acceleration, e.g. cuda if you have compatible nvidia-gpu.*\n",
    "\n",
    "Below are commands to install within current jupiter environment.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu113\n",
    "!pip install transformers datasets\n",
    "!pip install scikit-learn\n",
    "!pip install tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Let's set up constants"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "cache_dir = 'D:/cache/huggingface'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Loading pretrained model\n",
    "As focus for this model is to work with NED task on Polish data, we need to use pretrained model which is either multilingual or trained on Polish data.\n",
    "Currently, one of the best pretrained language models for Polish language is HerBERT. You can find more information in its model card: https://huggingface.co/allegro/herbert-base-cased\n",
    "HerBERT is BERT based language model. We will train it to perform Token Classification task, so we should use *BertForTokenClassification* class from *transformers*.\n",
    "\n",
    "We will try to predict 3 classes in *IOB* format. Also, we should set up dropout to prevent overfitting."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allegro/herbert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.sso.sso_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.sso.sso_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at allegro/herbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, HerbertTokenizerFast\n",
    "\n",
    "name = \"allegro/herbert-base-cased\"\n",
    "\n",
    "\n",
    "tokenizer = HerbertTokenizerFast.from_pretrained(\n",
    "    name, cache_dir=cache_dir\n",
    ")\n",
    "model: BertForTokenClassification = BertForTokenClassification.from_pretrained(\n",
    "    name, cache_dir=cache_dir, num_labels=3,\n",
    "    attention_probs_dropout_prob=0.3,\n",
    "    hidden_dropout_prob=0.3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use scheduled training starting with training auxiliary layers of classifier first. During training we will extend gradient updates down into language models weights. This way we can finetune not only classifier but also language model - still using GPU with relatively small 4GB VRAM."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models classifier:  Linear(in_features=768, out_features=3, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# We can check now size of our classifier\n",
    "print(\"Models classifier: \", model.classifier)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Preparing dataset\n",
    "Let's start with downloading Polish NER dataset. Dataset we will be using is called *kpwr-ner* we can check more information about it, on its dataset card on huggingface website: https://huggingface.co/datasets/clarin-pl/kpwr-ner"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset kpwrner (D:/cache/huggingface\\clarin-pl___kpwrner\\default\\0.0.0\\001e3d471298007e8412e3a6ccc06bec000dec1bce0cf8e0ba7e5b7e105b1342)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2e65025ab2304f92a9a664a47c37acf6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "kpwr_set = load_dataset(\"clarin-pl/kpwr-ner\", cache_dir=cache_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Each token is tagged in IOB format.\n",
    "*O* means *Other* token - outside of phrase.\n",
    "*B-* means *Beginning* - first token of phrase.\n",
    "*I-* means *Inner* - second or subsequent token in phrase.\n",
    "Tokens *B-* and *I-* also contain information about class of given phrase for example: *B-nam_liv_person*  and *I-nam_liv_person* for name of person, or *B-nam_loc_gpe_city* and *I-nam_loc_gpe_city* for name of the geographical location - city to be exact, etc.\n",
    "We need to change those tokens into 3 tokens that we want to predict:\n",
    "*O* - token will be indexed as *0*\n",
    "*B* - token will be indexed as *1*\n",
    "*I* - token will be indexed as *2*\n",
    "As HerBERT does not use word tokenizer, but sub-word tokenizer - we will also encounter special token tag *-100*, which means given token is a subsequent sub-word token not first sub-word in word representation."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/14 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "754e93221efb4fcfba83affc261e4f95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:/cache/huggingface\\clarin-pl___kpwrner\\default\\0.0.0\\001e3d471298007e8412e3a6ccc06bec000dec1bce0cf8e0ba7e5b7e105b1342\\cache-f075577aa7c293af.arrow\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# Casting NER to NED format\n",
    "def cast_ner_to_ned(tag_i):\n",
    "    tag = kpwr_set['train'].features['ner'].feature.int2str(tag_i)\n",
    "    if 'b' == tag[0].lower():\n",
    "        return 1\n",
    "    if 'i' == tag[0].lower():\n",
    "        return 2\n",
    "\n",
    "    assert tag.lower() == 'o'\n",
    "    return 0\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner\"]):\n",
    "        # Map tokens to their respective word.\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Only label the first token of a given word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(cast_ner_to_ned(label[word_idx]))\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_kwpr = kpwr_set.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from transformers import TrainingArguments, Trainer, SchedulerType, IntervalStrategy\n",
    "from transformers.integrations import TensorBoardCallback\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    ignore_index = labels == -100\n",
    "    other_pred = np.logical_or(predictions == 0, ignore_index)\n",
    "    other_label = np.logical_or(labels == 0, ignore_index)\n",
    "    hit_others = np.logical_not(np.logical_and(other_label, other_pred))\n",
    "    acc = accuracy_score(labels[hit_others], predictions[hit_others])\n",
    "\n",
    "    f1 = f1_score(labels[hit_others], predictions[hit_others], average='macro')\n",
    "\n",
    "    return {'accuracy': acc, 'f1': f1}\n",
    "\n",
    "\n",
    "batch_size = 20\n",
    "save_path = 'D:/models/ned'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_path,\n",
    "    evaluation_strategy=IntervalStrategy.EPOCH,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size*2,\n",
    "    num_train_epochs=25,\n",
    "    lr_scheduler_type=SchedulerType.LINEAR,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    "    save_steps=200,\n",
    "    logging_steps=100,\n",
    "    learning_rate=1\n",
    ")\n",
    "\n",
    "model_size = 512\n",
    "warmup = 1000\n",
    "\n",
    "\n",
    "def lambda_lr(step):\n",
    "    step += 1\n",
    "    if step == 2600:\n",
    "        print(\"Training layer 11.\")\n",
    "        for param in model.bert.encoder.layer[11].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if step == 5400:\n",
    "        print(\"Training layer 10.\")\n",
    "        for param in model.bert.encoder.layer[10].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if step == 7600:\n",
    "        print(\"Training layer 9.\")\n",
    "        for param in model.bert.encoder.layer[9].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    if step == 10000:\n",
    "        print(\"Training layer 8.\")\n",
    "        for param in model.bert.encoder.layer[9].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    return model_size**(-0.5)*(min(step ** (-0.5), step * warmup ** (-1.5)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are adding a <class 'transformers.integrations.TensorBoardCallback'> to the callbacks of this Trainer, but there is already one. The currentlist of callbacks is\n",
      ":DefaultFlowCallback\n",
      "TensorBoardCallback\n",
      "NotebookProgressCallback\n"
     ]
    }
   ],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def create_scheduler(\n",
    "            self, num_training_steps: int,\n",
    "            optimizer: torch.optim.Optimizer = None\n",
    "    ):\n",
    "        optimizer = self.optimizer if optimizer is None else optimizer\n",
    "        self.lr_scheduler = LambdaLR(optimizer, lambda_lr, -1)\n",
    "        return self.lr_scheduler\n",
    "\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_kwpr[\"train\"],\n",
    "    eval_dataset=tokenized_kwpr[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.add_callback(TensorBoardCallback())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running training *****\n",
      "  Num examples = 13959\n",
      "  Num Epochs = 25\n",
      "  Instantaneous batch size per device = 20\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 20\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 17450\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='17450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    2/17450 : < :, Epoch 0.00/25]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:/models/ned\\checkpoint-200\n",
      "Configuration saved in D:/models/ned\\checkpoint-200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-400\n",
      "Configuration saved in D:/models/ned\\checkpoint-400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-600\n",
      "Configuration saved in D:/models/ned\\checkpoint-600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-600\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-800\n",
      "Configuration saved in D:/models/ned\\checkpoint-800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-1000\n",
      "Configuration saved in D:/models/ned\\checkpoint-1000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-1000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-1000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-1000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-1200\n",
      "Configuration saved in D:/models/ned\\checkpoint-1200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-1200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-1200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-1200\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-1400\n",
      "Configuration saved in D:/models/ned\\checkpoint-1400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-1400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-1400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-1400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-1600\n",
      "Configuration saved in D:/models/ned\\checkpoint-1600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-1600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-1600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-1600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-1800\n",
      "Configuration saved in D:/models/ned\\checkpoint-1800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-1800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-1800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-1800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-2000\n",
      "Configuration saved in D:/models/ned\\checkpoint-2000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-2000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-2000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-2000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-2200\n",
      "Configuration saved in D:/models/ned\\checkpoint-2200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-2200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-2200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-2200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-2400\n",
      "Configuration saved in D:/models/ned\\checkpoint-2400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-2400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-2400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-2400\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer 11.\n",
      "Training layer 11.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:/models/ned\\checkpoint-2600\n",
      "Configuration saved in D:/models/ned\\checkpoint-2600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-2600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-2600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-2600\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-2800\n",
      "Configuration saved in D:/models/ned\\checkpoint-2800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-2800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-2800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-2800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-3000\n",
      "Configuration saved in D:/models/ned\\checkpoint-3000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-3000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-3000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-3000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-3200\n",
      "Configuration saved in D:/models/ned\\checkpoint-3200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-3200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-3200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-3200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-3400\n",
      "Configuration saved in D:/models/ned\\checkpoint-3400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-3400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-3400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-3400\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-3600\n",
      "Configuration saved in D:/models/ned\\checkpoint-3600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-3600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-3600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-3600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-3800\n",
      "Configuration saved in D:/models/ned\\checkpoint-3800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-3800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-3800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-3800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-4000\n",
      "Configuration saved in D:/models/ned\\checkpoint-4000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-4000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-4000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-4000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-4200\n",
      "Configuration saved in D:/models/ned\\checkpoint-4200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-4200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-4200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-4200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-4400\n",
      "Configuration saved in D:/models/ned\\checkpoint-4400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-4400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-4400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-4400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-4600\n",
      "Configuration saved in D:/models/ned\\checkpoint-4600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-4600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-4600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-4600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-4800\n",
      "Configuration saved in D:/models/ned\\checkpoint-4800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-4800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-4800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-4800\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-5000\n",
      "Configuration saved in D:/models/ned\\checkpoint-5000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-5000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-5000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-5000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-5200\n",
      "Configuration saved in D:/models/ned\\checkpoint-5200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-5200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-5200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-5200\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer 10.\n",
      "Training layer 10.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:/models/ned\\checkpoint-5400\n",
      "Configuration saved in D:/models/ned\\checkpoint-5400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-5400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-5400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-5400\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-5600\n",
      "Configuration saved in D:/models/ned\\checkpoint-5600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-5600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-5600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-5600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-5800\n",
      "Configuration saved in D:/models/ned\\checkpoint-5800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-5800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-5800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-5800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-6000\n",
      "Configuration saved in D:/models/ned\\checkpoint-6000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-6000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-6000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-6000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-6200\n",
      "Configuration saved in D:/models/ned\\checkpoint-6200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-6200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-6200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-6200\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-6400\n",
      "Configuration saved in D:/models/ned\\checkpoint-6400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-6400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-6400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-6400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-6600\n",
      "Configuration saved in D:/models/ned\\checkpoint-6600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-6600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-6600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-6600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-6800\n",
      "Configuration saved in D:/models/ned\\checkpoint-6800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-6800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-6800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-6800\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-7000\n",
      "Configuration saved in D:/models/ned\\checkpoint-7000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-7000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-7000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-7000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-7200\n",
      "Configuration saved in D:/models/ned\\checkpoint-7200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-7200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-7200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-7200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-7400\n",
      "Configuration saved in D:/models/ned\\checkpoint-7400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-7400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-7400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-7400\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer 9.\n",
      "Training layer 9.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:/models/ned\\checkpoint-7600\n",
      "Configuration saved in D:/models/ned\\checkpoint-7600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-7600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-7600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-7600\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-7800\n",
      "Configuration saved in D:/models/ned\\checkpoint-7800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-7800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-7800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-7800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-8000\n",
      "Configuration saved in D:/models/ned\\checkpoint-8000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-8000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-8000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-8000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-8200\n",
      "Configuration saved in D:/models/ned\\checkpoint-8200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-8200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-8200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-8200\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-8400\n",
      "Configuration saved in D:/models/ned\\checkpoint-8400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-8400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-8400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-8400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-8600\n",
      "Configuration saved in D:/models/ned\\checkpoint-8600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-8600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-8600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-8600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-8800\n",
      "Configuration saved in D:/models/ned\\checkpoint-8800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-8800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-8800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-8800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-9000\n",
      "Configuration saved in D:/models/ned\\checkpoint-9000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-9000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-9000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-9000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-9200\n",
      "Configuration saved in D:/models/ned\\checkpoint-9200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-9200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-9200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-9200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-9400\n",
      "Configuration saved in D:/models/ned\\checkpoint-9400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-9400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-9400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-9400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-9600\n",
      "Configuration saved in D:/models/ned\\checkpoint-9600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-9600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-9600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-9600\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-9800\n",
      "Configuration saved in D:/models/ned\\checkpoint-9800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-9800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-9800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-9800\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer 8.\n",
      "Training layer 8.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to D:/models/ned\\checkpoint-10000\n",
      "Configuration saved in D:/models/ned\\checkpoint-10000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-10000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-10000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-10000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-10200\n",
      "Configuration saved in D:/models/ned\\checkpoint-10200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-10200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-10200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-10200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-10400\n",
      "Configuration saved in D:/models/ned\\checkpoint-10400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-10400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-10400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-10400\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-10600\n",
      "Configuration saved in D:/models/ned\\checkpoint-10600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-10600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-10600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-10600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-10800\n",
      "Configuration saved in D:/models/ned\\checkpoint-10800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-10800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-10800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-10800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-11000\n",
      "Configuration saved in D:/models/ned\\checkpoint-11000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-11000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-11000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-11000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-11200\n",
      "Configuration saved in D:/models/ned\\checkpoint-11200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-11200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-11200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-11200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-11400\n",
      "Configuration saved in D:/models/ned\\checkpoint-11400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-11400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-11400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-11400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-11600\n",
      "Configuration saved in D:/models/ned\\checkpoint-11600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-11600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-11600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-11600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-11800\n",
      "Configuration saved in D:/models/ned\\checkpoint-11800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-11800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-11800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-11800\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-12000\n",
      "Configuration saved in D:/models/ned\\checkpoint-12000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-12000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-12000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-12000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-12200\n",
      "Configuration saved in D:/models/ned\\checkpoint-12200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-12200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-12200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-12200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-12400\n",
      "Configuration saved in D:/models/ned\\checkpoint-12400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-12400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-12400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-12400\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-12600\n",
      "Configuration saved in D:/models/ned\\checkpoint-12600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-12600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-12600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-12600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-12800\n",
      "Configuration saved in D:/models/ned\\checkpoint-12800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-12800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-12800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-12800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-13000\n",
      "Configuration saved in D:/models/ned\\checkpoint-13000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-13000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-13000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-13000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-13200\n",
      "Configuration saved in D:/models/ned\\checkpoint-13200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-13200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-13200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-13200\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-13400\n",
      "Configuration saved in D:/models/ned\\checkpoint-13400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-13400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-13400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-13400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-13600\n",
      "Configuration saved in D:/models/ned\\checkpoint-13600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-13600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-13600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-13600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-13800\n",
      "Configuration saved in D:/models/ned\\checkpoint-13800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-13800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-13800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-13800\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-14000\n",
      "Configuration saved in D:/models/ned\\checkpoint-14000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-14000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-14000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-14000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-14200\n",
      "Configuration saved in D:/models/ned\\checkpoint-14200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-14200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-14200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-14200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-14400\n",
      "Configuration saved in D:/models/ned\\checkpoint-14400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-14400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-14400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-14400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-14600\n",
      "Configuration saved in D:/models/ned\\checkpoint-14600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-14600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-14600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-14600\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-14800\n",
      "Configuration saved in D:/models/ned\\checkpoint-14800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-14800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-14800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-14800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-15000\n",
      "Configuration saved in D:/models/ned\\checkpoint-15000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-15000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-15000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-15000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-15200\n",
      "Configuration saved in D:/models/ned\\checkpoint-15200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-15200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-15200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-15200\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-15400\n",
      "Configuration saved in D:/models/ned\\checkpoint-15400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-15400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-15400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-15400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-15600\n",
      "Configuration saved in D:/models/ned\\checkpoint-15600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-15600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-15600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-15600\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-15800\n",
      "Configuration saved in D:/models/ned\\checkpoint-15800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-15800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-15800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-15800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-16000\n",
      "Configuration saved in D:/models/ned\\checkpoint-16000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-16000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-16000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-16000\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-16200\n",
      "Configuration saved in D:/models/ned\\checkpoint-16200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-16200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-16200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-16200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-16400\n",
      "Configuration saved in D:/models/ned\\checkpoint-16400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-16400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-16400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-16400\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-16600\n",
      "Configuration saved in D:/models/ned\\checkpoint-16600\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-16600\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-16600\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-16600\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-16800\n",
      "Configuration saved in D:/models/ned\\checkpoint-16800\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-16800\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-16800\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-16800\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-17000\n",
      "Configuration saved in D:/models/ned\\checkpoint-17000\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-17000\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-17000\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-17000\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-17200\n",
      "Configuration saved in D:/models/ned\\checkpoint-17200\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-17200\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-17200\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-17200\\special_tokens_map.json\n",
      "Saving model checkpoint to D:/models/ned\\checkpoint-17400\n",
      "Configuration saved in D:/models/ned\\checkpoint-17400\\config.json\n",
      "Model weights saved in D:/models/ned\\checkpoint-17400\\pytorch_model.bin\n",
      "tokenizer config file saved in D:/models/ned\\checkpoint-17400\\tokenizer_config.json\n",
      "Special tokens file saved in D:/models/ned\\checkpoint-17400\\special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: lemmas, tokens, ner, orth.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4323\n",
      "  Batch size = 40\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": "TrainOutput(global_step=17450, training_loss=0.11620779499283493, metrics={'train_runtime': 4878.3056, 'train_samples_per_second': 71.536, 'train_steps_per_second': 3.577, 'total_flos': 1.108358648736525e+16, 'train_loss': 0.11620779499283493, 'epoch': 25.0})"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Let's evaluate fine-tuned model on"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Re-loading dataset tokenizer and imports."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset kpwrner (D:/cache/huggingface\\clarin-pl___kpwrner\\default\\0.0.0\\001e3d471298007e8412e3a6ccc06bec000dec1bce0cf8e0ba7e5b7e105b1342)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dd68a157d1c34e9fbf87136721612b94"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at D:/cache/huggingface\\clarin-pl___kpwrner\\default\\0.0.0\\001e3d471298007e8412e3a6ccc06bec000dec1bce0cf8e0ba7e5b7e105b1342\\cache-70c2f2ddf1b4e5e6.arrow\n",
      "Loading cached processed dataset at D:/cache/huggingface\\clarin-pl___kpwrner\\default\\0.0.0\\001e3d471298007e8412e3a6ccc06bec000dec1bce0cf8e0ba7e5b7e105b1342\\cache-654642d003d6e704.arrow\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DataCollatorForTokenClassification,BertForTokenClassification, HerbertTokenizerFast\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "save_path = 'D:/models/ned/checkpoint-15800'\n",
    "cache_dir = 'D:/cache/huggingface'\n",
    "name = \"allegro/herbert-base-cased\"\n",
    "\n",
    "kpwr_set = load_dataset(\"clarin-pl/kpwr-ner\", cache_dir=cache_dir)\n",
    "\n",
    "tokenizer = HerbertTokenizerFast.from_pretrained(\n",
    "    save_path\n",
    ")\n",
    "# Casting NER to NED format\n",
    "def cast_ner_to_ned(tag_i):\n",
    "    tag = kpwr_set['train'].features['ner'].feature.int2str(tag_i)\n",
    "    if 'b' == tag[0].lower():\n",
    "        return 1\n",
    "    if 'i' == tag[0].lower():\n",
    "        return 2\n",
    "\n",
    "    assert tag.lower() == 'o'\n",
    "    return 0\n",
    "\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner\"]):\n",
    "        # Map tokens to their respective word.\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # Only label the first token of a given word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(cast_ner_to_ned(label[word_idx]))\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "tokenized_kwpr = kpwr_set.map(tokenize_and_align_labels, batched=True)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model: BertForTokenClassification = BertForTokenClassification.from_pretrained(save_path)\n",
    "test_set = tokenized_kwpr['test']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W koÅ„cu wyszÅ‚o , do czego potrzebne byÅ‚o Google Gears .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']\n",
      "\n",
      "\n",
      "Dorzucono kilka bardziej smakowitych kÄ…skÃ³w i mamy wreszcie system operacyjny przeglÄ…darkÄ™ Google Chrome .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']\n",
      "\n",
      "\n",
      "NiezaleÅ¼nie od tego , czy bÄ™dzie to Å›miertelny cios dla Windows czy dla Firefoksa , program jest kolejnym zwiastunem zmian w interfejsie graficznym .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Po kilku minutach rzeczywiÅ›cie da siÄ™ odczuÄ‡ szybkoÅ›Ä‡ wczytywania stron z JavaScriptem , a kaÅ¼da zakÅ‚adka w oddzielnym procesie teÅ¼ brzmi nieÅºle ( szczegÃ³lnie pod Linuksem * , w ktÃ³rym Adobe Flash regularnie wywala przeglÄ…darki - niestety wersji dla Linuksa na razie brak ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Beznadziejnie na pierwszy rzut oka wyglÄ…dajÄ… zakÅ‚adki ( foldery ? ! ) , ale byÄ‡ moÅ¼e trzymanie zakÅ‚adek w przeglÄ…darce jest juÅ¼ niemodne .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "SwojÄ… drogÄ… integracji z del.icio.us teÅ¼ nie widaÄ‡ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Motywacje Google teÅ¼ sÄ… raczej jasne ( konwergencja , znowu ) - gdy Chrome ustawimy jako domyÅ›lnÄ… przeglÄ…darkÄ™ w Windows , nazwa \" Internet \" w menu start nabierze wÅ‚aÅ›ciwego znaczenia . . .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Chrome wyglÄ…da Å‚adnie .\n",
      "True tags:\n",
      "['B', 'I', 'I', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "W nowym Firefoksie jest AwesomeBar , podobny pasek adresu jest teÅ¼ w Chrome .\n",
      "True tags:\n",
      "['O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O']\n",
      "\n",
      "\n",
      "Niby to nic wielkiego - uÅ¼ytkownicy Apple od dawna majÄ… Spotlight ( zresztÄ… - to co w Windowsie nazywa siÄ™ \" Explorer \" , na Maku od dawna nazywa siÄ™ \" Finder \" ) , w ViÅ›cie teÅ¼ jest nieÅ›miaÅ‚e okienko wyszukiwania ( choÄ‡ prezentacja wynikÃ³w w postaci nudnej listy ) , Google Desktop istnieje juÅ¼ dÅ‚ugo , a w Linuksie gÅ‚owa wrÄ™cz boli od przybytku ( Tracker , Beagle , Deskbar ) , ale jedno jest pewne : rola pisania w interfejsie graficznym siÄ™ zwiÄ™ksza .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'B', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "JeÅ›li AwesomeBar jest za maÅ‚o Awesome - moÅ¼na zainstalowaÄ‡ Ubiquity .\n",
      "True tags:\n",
      "['O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O']\n",
      "\n",
      "\n",
      "JeÅ›li tekstowa obsÅ‚uga przeglÄ…darki to za maÅ‚o - na Maku jest Quicksilver , w Linuksie Gnome Do ( w Windowsie na pewno teÅ¼ coÅ› jest ) : piszesz i wybierasz z listy , program zapamiÄ™tuje zachowania uÅ¼ytkownika , czasem wystarczy kilka liter .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Szukanie nie sÅ‚uÅ¼y juÅ¼ tylko znalezieniu rzeczy , ktÃ³re gdzieÅ› siÄ™ zapodziaÅ‚y , szukanie zastÄ™puje przeglÄ…danie .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PrzecieÅ¼ przed pojawieniem siÄ™ Google , internet byÅ‚ katalogowany , dopiero pÃ³Åºniej zostaÅ‚ tak naprawdÄ™ zindeksowany .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "ByÄ‡ moÅ¼e to stara forma internetu byÅ‚a przyczynÄ… poraÅ¼ki microsoftowego Active Desktop .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O']\n",
      "\n",
      "\n",
      "W koÅ„cu kiepskie wykonanie , wymuszanie niestandardowych formatÃ³w i zasoboÅ¼ernoÅ›Ä‡ innym produktom MS nie zaszkodziÅ‚y . . .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Tymczasem teraz nasze dyski sÄ… juÅ¼ zindeksowane , czas wyrobiÄ‡ odpowiednie nawyki .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "A wtedy granica miÄ™dzy online i offline zatrze siÄ™ jeszcze bardziej .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "I kto na tym skorzysta ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "* Firma Google zapisuje TwÃ³j adres w celu przesyÅ‚ania wiadomoÅ›ci dotyczÄ…cych przeglÄ…darki Google Chrome oraz inforlinuxji o aktualizacjach i wydaniu gotowej wersji .\n",
      "True tags:\n",
      "['O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Podanie adresu oznacza wyraÅ¼enie zgody na otrzymywanie e - maili zawierajÄ…cych inforlinuxje tego typu .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Google bÄ™dzie przechowywaÄ‡ TwÃ³j adres e - mail przez pewien czas po wydaniu przeglÄ…darki Google Chrome dla systemu Linux , a nastÄ™pnie go usunie .\n",
      "True tags:\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'B', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "WiÄ™cej inforlinuxji na temat przechowywania danych uÅ¼ytkownikÃ³w przez Google moÅ¼na znaleÅºÄ‡ w zasadach ochrony prywatnoÅ›ci .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Subiektywny przeglÄ…d kaw z mlekiem i rogalikÃ³w .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Rogalik spÅ‚aszczony przez toster , cukier z wierzchu lekko przypalony , w stylu crema catalana .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "CiepÅ‚y , podany ze sztuÄ‡cami .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PociÄ™ty w paski .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Rogalik pociÄ™ty w paski , na zimno .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Podany ze sztuÄ‡cami .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Zjadany w centrum przed wyjazdem autobusu na plaÅ¼Ä™ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Rogalik zwykÅ‚y i wymiÄ™toszony , do rÄ™ki w serwetce , kawa w szczycie dÅ‚ugiej przerwy podana w plastikowym kubku .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Ale za to caÅ‚y zestaw za marne 1 euro .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "\n",
      "StoÅ‚Ã³wka uniwersytecka przerabia kawÄ™ i rogaliki w tempie kawiarni na duÅ¼ym dworcu .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Kawa z mlekiem ciepÅ‚ym lub zimnym , cortado ( maÅ‚o mleka ) , cortado z mlekiem skondensowanym ( ale naprawdÄ™ skondensowanym , cortado podaje siÄ™ w maÅ‚ych szklaneczkach , na dnie gruba warstwa mleka , trzeba energicznie wymieszaÄ‡ ) , kawa ( czyli solo , czyli espresso ) . . .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wszystko po piÄ™Ä‡dziesiÄ…t centÃ³w .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'B', 'B', 'O']\n",
      "\n",
      "\n",
      "Inna uniwersytecka stoÅ‚Ã³wka , 100 metrÃ³w dalej , ale ze stolikami na dworze .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Ceny zupeÅ‚nie inne - kawa z mlekiem i rogalik za 1 , 45 euro , rÃ³Å¼ne ceny na rÃ³Å¼ne rodzaje kaw .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Ale siedzenie na sÅ‚oÅ„cu , w grudniu - bezcenne .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "I jeszcze Å›rodek niedzieli - Alek Tarkowski zwrÃ³ciÅ‚ uwagÄ™ na przejmowanie przez banki przestrzeni publicznej w Warszawie .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "\n",
      "W niedzielÄ™ , okoÅ‚o godziny 14 zachciaÅ‚o nam siÄ™ wyjÅ›Ä‡ na kawÄ™ , a dzielnica jest dopiero w budowie ( ma Å‚Ä…czyÄ‡ wybudowany w szczerym polu uniwersytet oraz miasto ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Na rondzie przy gÅ‚Ã³wnym wjeÅºdzie na teren uniwersytetu sÄ… dwa banki .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Na ulicy , ktÃ³ra bÄ™dzie prowadziÄ‡ do miasta , sÄ… jeszcze dwa , mimo Å¼e nie da siÄ™ niÄ… na razie jeÅºdziÄ‡ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Ale kawÄ™ ( bez rogalika ) udaÅ‚o nam siÄ™ znaleÅºÄ‡ w jednym z dziewiÄ™ciu barÃ³w , ktÃ³re znajdujÄ… siÄ™ promieniu 500 metrÃ³w od mieszkania ( wiÄ™kszoÅ›Ä‡ byÅ‚a zamkniÄ™ta , bo niedziela / sjesta , jeden nam siÄ™ nie podobaÅ‚ itp . ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wszystkie budynki ( z ktÃ³rych znaczna czÄ™Å›Ä‡ nie jest zamieszkana ) majÄ… przeznaczone miejsce na usÅ‚ugi / sklepy , wiÄ™c moÅ¼na siÄ™ spodziewaÄ‡ , Å¼e za rok bÄ™dzie juÅ¼ moÅ¼na wybieraÄ‡ spoÅ›rÃ³d 20 barÃ³w , rogalikÃ³w na ciepÅ‚o i na zimno , rogalikÃ³w pociÄ™tych w paseczki . . .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Koniec napisÃ³w .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "JuÅ¼ od dawna trwaÅ‚a walka z amatorami tworzÄ…cymi napisy do filmÃ³w , teraz skoÅ„czyÅ‚ siÄ™ czas polemik w gazecie , pora na salÄ™ sÄ…dowÄ… .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "OczywiÅ›cie to tylko jedno ÅºrÃ³dÅ‚o napisÃ³w , ale \" sprawa jest rozwojowa \" .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Åatwiej bÄ™dzie zapewne nauczyÄ‡ siÄ™ angielskiego , niemieckiego , francuskiego , hiszpaÅ„skiego , wÅ‚oskiego , japoÅ„skiego i czeskiego ( na poczÄ…tek powinno wystarczyÄ‡ ) niÅ¼ zmieniÄ‡ prawo autorskie .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'O', 'B', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Tymczasem ze wszystkich ogniw \" pirackiego \" Å‚aÅ„cucha tÅ‚umacze napisÃ³w sÄ… pewnie najmniej groÅºni , ale za to najÅ‚atwiejsi do wyÅ›ledzenia .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "I jak zabierze siÄ™ napisy , na pewno spadnie liczba Å›ciÄ…ganych i udostÄ™pnianych filmÃ³w .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Na pewno .\n",
      "True tags:\n",
      "['O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O']\n",
      "\n",
      "\n",
      "W okolicy obowiÄ…zkowo jest grÃ³b Hamleta , wiatraki , a domy kryte sÄ… strzechÄ… lub azbestem .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "KaÅ¼dy nosi ze sobÄ… termos , z prohibicjÄ… ma on jednak niewiele wspÃ³lnego , w Å›rodku jest kawa , bez ktÃ³rej przeciÄ™tny DuÅ„czyk nie przetrwaÅ‚ by nawet godziny .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Kawa jest smolista , raczej przyzwoita i jak na polskie standardy raczej mocna , chociaÅ¼ fani espresso mogÄ… poczuÄ‡ siÄ™ obraÅ¼eni takim stwierdzeniem .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "KawÄ™ podaje siÄ™ przy kaÅ¼dej okazji , rÃ³wnieÅ¼ w porze na kawÄ™ ( o 15 ) , do kawy sÄ… wÃ³wczas buÅ‚ki z serem Å¼Ã³Å‚tym i marmoladÄ… , ewentualnie ciasto .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Mleka do kawy siÄ™ nie dolewa , w koÅ„cu jest w serze .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Ser to tylko jeden element deja vu , ktÃ³re napadÅ‚o mnie w Danii , gdzie zaczÄ…Å‚ em czytaÄ‡ nazwy zgodnie z wymowÄ… holenderskÄ… .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Kawa zresztÄ… teÅ¼ podobna , wioski teÅ¼ jak z Might and Magic , szosy peÅ‚ne opli astra i fordÃ³w focusÃ³w , nawet zioÅ‚owa wÃ³dka taka sama .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "I oczywiÅ›cie ser z kminkiem .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Goudse belegen met komijn byÅ‚ zawsze moim holenderskim faworytem , w Danii sery rÃ³wnieÅ¼ wystÄ™pujÄ… w rÃ³Å¼nym stopniu dojrzaÅ‚oÅ›ci , a dojrzaÅ‚y danbo . . .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wystarczy uchyliÄ‡ lodÃ³wkÄ™ , Å¼eby wiedzieÄ‡ , Å¼e zostaÅ‚ jeszcze kawaÅ‚ek .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Szkoda , Å¼e taki maÅ‚y .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "W Danii trafiÅ‚ em akurat na ekspresowe Å¼niwa , caÅ‚y lipiec padaÅ‚o , musieli nadrabiaÄ‡ .\n",
      "True tags:\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PatrzÄ…c na gigantyczne traktory i imperialne kombajny trudno zrozumieÄ‡ duÅ„ski dystans wobec Unii Europejskiej .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'B', 'I', 'O']\n",
      "\n",
      "\n",
      "ChociaÅ¼ rozmawiaÅ‚ em z mÅ‚odym rolnikiem , ktÃ³ry ma dwadzieÅ›cia hektarÃ³w , biegle mÃ³wi po angielsku i na wakacje jeÅºdzi do Austrii na narty .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Uprawia ziemiÄ™ , bo lubi , uwaÅ¼a to za ciekawÄ… pracÄ™ i nie ma nic do UE .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "\n",
      "ZresztÄ… duÅ„scy rolnicy , nie tylko mÅ‚odzi , wyposaÅ¼eni sÄ… w komputery i staÅ‚e Å‚Ä…cza , na dodatek potrafiÄ… z nich korzystaÄ‡ .\n",
      "True tags:\n",
      "['O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "MajÄ… swoje strony internetowe , wprawdzie na etapie 1 . 0 , nikt mnie do facebooka nie zaprosiÅ‚ , ale sieciowe umiejÄ™tnoÅ›ci sÄ… tam nieporÃ³wnywalnie wiÄ™ksze niÅ¼ w Polsce .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'I', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "\n",
      "Poza tym Dania to spokojne wakacje ( jeÅ›li zignorujemy to , Å¼e nasz domek wakacyjny moÅ¼e byÄ‡ pokryty azbestem ) , byÅ‚ em w szczycie sezonu , w weekend , pogoda momentami niezÅ‚a , a plaÅ¼e i deptaki niemal puste .\n",
      "True tags:\n",
      "['O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "We wszystkich wioskach osiedla domkÃ³w letniskowych , kemping co dwa kilometry , ale znikÄ…d nie dobiega basowe dudnienie , moÅ¼e byÅ‚ em na zbyt gÅ‚Ä™bokiej prowincji , a moÅ¼e rozrywkowi Niemcy , DuÅ„czycy i Holendrzy jeÅ¼dÅ¼Ä… raczej do Hiszpanii .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'O', 'B', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'B', 'O', 'B', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "\n",
      "Morza szum ( w koÅ„cu byÅ‚ em nad BaÅ‚tykiem ) , tÅ‚uste jedzenie - bo oprÃ³cz serÃ³w wystÄ™puje sporo miÄ™sa , kotletÃ³w mielonych w rÃ³Å¼nych postaciach , boczku w grubych plastrach , piwo , ktÃ³re niegdyÅ› sprowadzaÅ‚o siÄ™ do dwÃ³ch rÃ³Å¼nych marek ( w tym tego w prawdopodobnie najlepszej butelce ) , a teraz rzekomo co tydzieÅ„ powstaje ( wskrzesza siÄ™ ? ) nowy lokalny browar , chociaÅ¼ trzeba uwaÅ¼aÄ‡ , bo ceny lokalnych specjaÅ‚Ã³w raczej nie dla nas , chyba Å¼e w przeliczeniu na procenty , bo wtedy niektÃ³rym piwom bliÅ¼ej zdecydowanie do wina , a smak bynajmniej nie przypomina polskich \" mocnych \" i powiewajÄ…ce wszÄ™dzie , przed domami , za domami i obok domÃ³w , duÅ„skie flagi .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Czy piosenka to samochÃ³d ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Problem , czy chroniÄ‡ informacje jak wÅ‚asnoÅ›Ä‡ materialnÄ… nie jest nowy .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Dla amerykaÅ„skiego prawnika , ktÃ³ry przerabia ekonomicznÄ… analizÄ™ prawa na studiach , to oczywistoÅ›Ä‡ .\n",
      "True tags:\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Polski prawnik na studiach miaÅ‚ moÅ¼e wprowadzenie do mikro i makro , wiÄ™c przy odrobinie szczÄ™Å›cia odrÃ³Å¼ni inflacjÄ™ od elastycznoÅ›ci popytu .\n",
      "True tags:\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "W zwiÄ…zku z tym w Polsce moÅ¼na zawsze liczyÄ‡ na to , Å¼e prawnik ZAiKSu powie , Å¼e przecieÅ¼ to oczywiste , Å¼e samochÃ³d i zdjÄ™cie to jest dokÅ‚adnie to samo .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Bo przyszedÅ‚ do niego zapÅ‚akany fotograf .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Bo gdyby chciaÅ‚ wejÅ›Ä‡ do czyjegoÅ› samochodu , to musiaÅ‚ by zapytaÄ‡ o zgodÄ™ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "MusiaÅ‚ by zapytaÄ‡ o zgodÄ™ wÅ‚aÅ›ciciela ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "A dlaczego samochÃ³d ma wÅ‚aÅ›ciciela ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Dlaczego o zgodÄ™ , oprÃ³cz uÅ¼ytkownika , nie trzeba pytaÄ‡ wiÄ™c producenta samochodu ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Albo koproducentÃ³w , producenta klamki , tapicerki ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "OczywiÅ›cie , szybko moÅ¼na znaleÅºÄ‡ wyjÄ…tki .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "SamochÃ³d wypoÅ¼yczony albo samochÃ³d w leasingu byÄ‡ moÅ¼e jest bardziej podobny do utworu , ktÃ³rego licencjÄ™ kupujemy .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Jest jednak pewna zasadnicza rÃ³Å¼nica , jak powiedziaÅ‚ Krzysztof Siewicz - nie ma zaczarowanego oÅ‚Ã³wka , ktÃ³rym moÅ¼emy ten wypoÅ¼yczony samochÃ³d sobie skopiowaÄ‡ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "WciÄ…Å¼ jeden samochÃ³d w tym samym czasie moÅ¼e pokonaÄ‡ jednÄ… trasÄ™ , zmieÅ›ci siÄ™ w nim okreÅ›lona liczba osÃ³b .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Dlaczego producenci nie chcieli by zmieniÄ‡ modelu sprzedaÅ¼y samochodÃ³w ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "MoÅ¼e powinni zaczÄ…Ä‡ lobbowaÄ‡ za takim rozwiÄ…zaniem ( oczywiÅ›cie , jak juÅ¼ uda im siÄ™ wylobbowaÄ‡ ratunek przed kryzysem ) , bo przecieÅ¼ nie ma nic lepszego niÅ¼ wÅ‚asnoÅ›Ä‡ dla producenta i ustawowo ograniczone prawa nabywcÃ³w .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Dlaczego nie wprowadziÄ‡ licencji na samochody zamiast wÅ‚asnoÅ›ci ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Albo licencji na wszystko ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Niech tylko pierwotny twÃ³rca - stolarz , murarz , Å›lusarz bÄ™dzie wÅ‚aÅ›cicielem , a uÅ¼ytkownik licencjobiorcÄ… .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "A jeÅ›li produkcja jest bardziej skomplikowana ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Nic prostszego - moÅ¼na wprowadziÄ‡ takie prawo , Å¼eby wspÃ³Å‚producentom rÃ³wnieÅ¼ przysÅ‚ugiwaÅ‚o wynagrodzenie .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Jak tego pilnowaÄ‡ ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Czy na pewno nabywca krzesÅ‚a nie korzysta z niego na imprezie publicznej ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "MoÅ¼e na imprezÄ™ przyszedÅ‚ ktoÅ› spoza krÄ™gu towarzyskiego ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Czy licencja obejmuje przewoÅ¼enie autostopowiczÃ³w ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "MoÅ¼na powoÅ‚aÄ‡ odpowiedniÄ… organizacjÄ™ , albo lepiej 1000 organizacji , ktÃ³re bÄ™dÄ… to sprawdzaÄ‡ i liczyÄ‡ , a nastÄ™pnie rozdzielaÄ‡ naleÅ¼ne tantiemy .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wtedy wszyscy wreszcie staniemy siÄ™ rentierami .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Czeczenia , nazwa ktÃ³ra czÄ™sto przewija siÄ™ w gazetach i telewizji , szaremu obywatelowi mÃ³wi niewiele .\n",
      "True tags:\n",
      "['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "WiÄ™kszoÅ›Ä‡ PolakÃ³w nawet nie wie gdzie ten kraj leÅ¼y i dlaczego walczy z RosjÄ… .\n",
      "True tags:\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "\n",
      "ChciaÅ‚ by m choÄ‡ trochÄ™ przybliÅ¼yÄ‡ problematykÄ™ Kaukazu i napiÄ™Ä‡ w tamtejszych republikach .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Kiedy 11 marca 1985 roku Biuro Polityczne KC KPZR powierzyÅ‚o obowiÄ…zki Sekretarza Generalnego KC PZPR MichaiÅ‚owi Gorbaczowowi nikt na Kaukazie nie zdawaÅ‚ sobie sprawy , Å¼e nadchodzi czas zmian .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'B', 'O', 'O', 'O', 'B', 'O', 'I', 'B', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Gorbaczow starajÄ…cy siÄ™ uratowaÄ‡ gospodarkÄ™ a takÅ¼e odnowiÄ‡ zmurszaÅ‚y aparat partyjny wprowadziÅ‚ gÅ‚asnost i pierestrojkÄ™ co jednak przyczyniÅ‚o siÄ™ do aktywizacji Å›rodowisk demokratycznych i wywoÅ‚aÅ‚o falÄ™ separatyzmÃ³w i zadawnionych antagonizmÃ³w nie tylko na Kaukazie .\n",
      "True tags:\n",
      "['B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O']\n",
      "\n",
      "\n",
      "NarÃ³d czeczeÅ„ski , przeÅ›ladowany za dÄ…Å¼enie do niepodlegÅ‚oÅ›ci poczuÅ‚ wiatr zmian w paÅºdzierniku 1987 roku , kiedy nieliczna grupa intelektualistÃ³w powoÅ‚aÅ‚a przy Komsomole w Groznym â€“ Stowarzyszenie â€ž Kaukaz â€ majÄ…ce poczÄ…tkowo formÄ™ forum dyskusyjnego .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Powodem powstania i aktywizacji dziaÅ‚aczy czeczeÅ„skich byÅ‚o poparcie dla reform Gorbaczowa a takÅ¼e dÄ…Å¼enie do wolnoÅ›ci sÅ‚owa , wyjaÅ›nienia tragicznej historii narodu czeczeÅ„skiego , odnowy kultur a takÅ¼e rozwiÄ…zania problemÃ³w narodowoÅ›ciowych w republice .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "W 1988 roku uksztaÅ‚towaÅ‚ siÄ™ ZwiÄ…zek Poparcia Pierestrojki , organizacja posiadajÄ…ca nieformalne poparcie Moskwy ( stÄ…d nazywani czÄ™sto â€ž nieformaÅ‚ami â€ ) organizujÄ…ca wiece w Groznym pod hasÅ‚em sprawiedliwoÅ›ci spoÅ‚ecznej .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PostÄ™pujÄ…ca jednak radykalizacja stanowiska ZwiÄ…zku zaniepokoiÅ‚o MoskwÄ™ , ktÃ³ra zakazaÅ‚a organizowania wiecÃ³w .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wiece â€ž nieformaÅ‚ow â€ staÅ‚y siÄ™ jednak staÅ‚ym elementem Å¼ycia publicznego Groznego , aktywizujÄ…c stopniowo caÅ‚y kraj .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PÃ³Åºnym latem 1988 roku wraz z przeksztaÅ‚ceniem siÄ™ ZwiÄ…zku we Front Ludowy Cz-I ASRR pojawiÅ‚y siÄ™ Å¼Ä…dania zmian we wÅ‚adzach republiki , co nastÄ…piÅ‚o wiosnÄ… 1989 roku .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'B', 'I', 'I', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "I Sekretarzem zostaÅ‚ Doku Zawgajew , pierwszy Czeczen na tym stanowisku w historii co wywoÅ‚aÅ‚o falÄ™ entuzjazmu .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wraz z wstÄ…pieniem Zawgajewa na stanowisku Sekretarza nastÄ…piÅ‚a liberalizacja Å¼ycia republiki , ktÃ³ra w lutym i marcu 1990 roku doprowadziÅ‚a do wyksztaÅ‚cenia siÄ™ dwÃ³ch obozÃ³w : konserwatywno - internacjonalistycznego zdominowanego przez Rosjan i nurt partyjnych liberaÅ‚Ã³w opowiadajÄ…cych siÄ™ za peÅ‚nÄ… autonomiÄ… i moÅ¼liwoÅ›ciÄ… wyjÅ›cia z ZSRR .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O']\n",
      "\n",
      "\n",
      "OgromnÄ… rolÄ™ w Å¼yciu CzeczenÃ³w odgrywa islam .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Za rzÄ…dÃ³w Zawgajewa udaÅ‚o siÄ™ utworzyÄ‡ Czeczeno - Inguski MuzuÅ‚maÅ„ski ZarzÄ…d Duchowny , ktÃ³rego zadaniem byÅ‚a odbudowa Å›wiadomoÅ›ci muzuÅ‚maÅ„skiej .\n",
      "True tags:\n",
      "['O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "W marcu 1990 r . odbyÅ‚y siÄ™ wybory deputowanych ludowych RFSRR i Cz-I ASRR .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'I', 'O']\n",
      "\n",
      "\n",
      "W Radzie znalazÅ‚a siÄ™ stosunkowo silna grupa opozycyjnych dziaÅ‚aczy , ktÃ³rzy utworzyli frakcjÄ™ pod nazwÄ… Inicjatywa Demokratyczna .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "\n",
      "\n",
      "PrzewodniczÄ…cym Rady zostaÅ‚ wybrany Zawgajew , co pozwoliÅ‚o na kontynuowanie liberalizacji Å¼ycia publicznego .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "ChoÄ‡ zazwyczaj jestem bardzo ostroÅ¼na i nie klikam we wszystko co popadnie , tym razem nie zachowaÅ‚a m czujnoÅ›ci rewolucyjnej â€“ kliknÄ™Å‚a m w URL z wiadomoÅ›ci odruchowo i bez zastanowienia .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "I tylko dziÄ™ki Firefoxowi nie weszÅ‚a m na stronÄ™ , ktÃ³ra usiÅ‚owaÅ‚a siÄ™ otworzyÄ‡ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PrzeglÄ…darka zidentyfikowaÅ‚a stronÄ™ jako niebezpiecznÄ… .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "SprawdziÅ‚a m dokÅ‚adniej adres pod linkiem â€“ ukryto w nim fragment â€“ secure-myspace.com/redirect.htm?blog.myspace.com/ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Poza podejrzanym przekierowaniem autentyczny adres nie zawiera myÅ›lnika tylko kropkÄ™ ( secure.myspace.com ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'I', 'I', 'I', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Z ciekawoÅ›ci odwiedziÅ‚a m kilka profili uÅ¼ytkownikÃ³w MySpace i w wielu komentarzach znalazÅ‚a m podobnÄ… wiadomoÅ›Ä‡ wysÅ‚anÄ… z kont rÃ³Å¼nych uÅ¼ytkownikÃ³w .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Przypuszczam , Å¼e caÅ‚a ta akcja dziaÅ‚a na zasadzie Å‚aÅ„cuszka ( tak , jak wiele â€ž robali â€ mailowych ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "JeÅ›li ktoÅ› otworzy stronÄ™ podanÄ… w linku hakerzy uzyskujÄ… dostÄ™p do jego konta i z niego rozsyÅ‚ajÄ… faÅ‚szywe komentarze do wszystkich przyjaciÃ³Å‚ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Moja wiedza na temat hakowania jest zerowa ( i nie odczuwam potrzeby szkolenia siÄ™ w tej dziedzinie ) zatem byÄ‡ moÅ¼e siÄ™ mylÄ™ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "W kaÅ¼dym razie uÅ¼ytkownikom MySpace radzÄ™ bardzo uwaÅ¼aÄ‡ na to w co klikajÄ… ( choÄ‡ pewnie wiÄ™kszoÅ›Ä‡ nie klika mechanicznie ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\" zamiast edukowaÄ‡ uÅ¼ytkownikÃ³w internetu , przyjÄ™to strategiÄ™ siania niepokoju i niszczenia konkurencji \" - na pewno nie ma Å¼adnych tekstÃ³w edukujÄ…cych ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "To sÄ… insynuacje .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\" oczywiÅ›cie , jak kaÅ¼da sÅ‚uÅ¼ba , giodo bÄ™dzie chciaÅ‚o siÄ™ wykazaÄ‡ \" - rzeczywiÅ›cie po ostatnich dwÃ³ch latach moÅ¼na odnieÅ›Ä‡ wraÅ¼enie , Å¼e sÅ‚uÅ¼by powoÅ‚ano wyÅ‚Ä…cznie w celu wykazywania siÄ™ , ale gdzie te liczne przykÅ‚ady naduÅ¼yÄ‡ GIODO ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O']\n",
      "\n",
      "\n",
      "\" ogÃ³lne przekaz jest prosty : \" - typowa Å‚asica .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "\" brakuje tylko pÅ‚aczÄ…cego dziecka i mieli by Å›my gazetowÄ… wersjÄ™ programu redaktor jaworowicz \" - jednak pÅ‚aczÄ…cego dziecka brakuje .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sytuacji nie ratuje tu ani forma wypowiedzi ( felieton ) , ani miejsce ( blog ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "JeÅ›li oskarÅ¼asz kogoÅ› o sianie zamÄ™tu , zbierasz burzÄ™ .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Sam fakt , Å¼e nie wiesz , czy Agora rzeczywiÅ›cie kazaÅ‚a swoim redaktorom napadaÄ‡ na NaszÄ… KlasÄ™ w ramach nieczystej walki rynkowej , ale to sugerujesz , a potem na TwÃ³j autorytet powoÅ‚uje siÄ™ Dziennik , Å¼eby rzeczywiÅ›cie Agorze dokopaÄ‡ ( co oczywiÅ›cie nie byÅ‚o Twoim zamiarem , a Dziennik olaÅ‚ prawo , o dobrych obyczajach nie wspominajÄ…c ) , jest czarnym PR , moÅ¼e rzeczywiÅ›cie bardziej niÅ¼ FUDem , ale metoda jest podobna .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Informacje sÄ… negatywne , sÄ… ogÃ³lne i majÄ… zdyskredytowaÄ‡ konkretny podmiot w konkretnym przypadku .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Masz oczywiÅ›cie racjÄ™ , Å¼e Wyborcza przesadziÅ‚a , Å¼e wyszedÅ‚ im FUD , nawet jeÅ¼eli wynikaÅ‚ z gÅ‚upoty redaktorÃ³w , ktÃ³rzy o internecie nie wiedzÄ… nic , a ochronie danych osobowych jeszcze mniej .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "GÅ‚upota ich nie usprawiedliwia .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Kontrola zbiorÃ³w danych jest jednak najwaÅ¼niejszym zadaniem GIODO , do tego caÅ‚y ten urzÄ…d zostaÅ‚ powoÅ‚any .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "OdbÄ™dzie siÄ™ nie dlatego , Å¼e Wyborcza napisaÅ‚a o przeraÅ¼onej prawniczce , tylko Wyborcza napisaÅ‚a o przeraÅ¼onej prawniczce ( podkreÅ›lam ponownie , Å¼e gÅ‚upio napisaÅ‚a ) , bo odbÄ™dzie siÄ™ kontrola .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Od razu zaznaczÄ™ , Å¼e filmu nie widziaÅ‚ em i chyba nie planujÄ™ , bo jestem podatny na sugestie , a paleniu podziÄ™kowaÅ‚ em kilka lat temu .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Nie interesuje mnie to , czy ludzie zaczynajÄ… paliÄ‡ wyÅ‚Ä…cznie w wyniku dziaÅ‚ania wolnej woli , presji otoczenia , spisku wielkich korporacji , reklam z sympatycznym wielbÅ‚Ä…dem czy nadmiaru wyÅ›cigÃ³w FormuÅ‚y 1 obejrzanych w mÅ‚odoÅ›ci .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "ZaÅ‚Ã³Å¼my , Å¼e jest to ich wolny wybÃ³r .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Palenie jednak produkuje znaczne negatywne efekty zewnÄ™trzne - w postaci kosztÃ³w opieki zdrowotnej ( efekt gapowicza - skoro wszyscy pÅ‚acÄ… tyle samo , to ja mogÄ™ pogorszyÄ‡ swoje zdrowie bardziej od innych ) , Å›mieci i pogorszenia siÄ™ samopoczucia osÃ³b niepalÄ…cych - i w zwiÄ…zku z tym wolnoÅ›Ä‡ palacza wchodzi w konflikt z wolnoÅ›ciÄ… niepalacza .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Przesuwanie tych granic jest takim samym zamachem na wolnoÅ›Ä‡ palaczy , jak ustalenie ich na dotychczasowym poziomie byÅ‚o zamachem na wolnoÅ›Ä‡ niepalaczy .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PrÃ³by rozwiÄ…zania problemu efektÃ³w zewnÄ™trznych obejmujÄ… dziaÅ‚ania z zakresu prawa ( ustawowe zakazy ) , rynku ( wysoka cena papierosÃ³w ) i moÅ¼e architektury ( o ile brak popielniczki moÅ¼e kogoÅ› zniechÄ™ciÄ‡ ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "A normy spoÅ‚eczne zakÅ‚adajÄ… tolerancjÄ™ ( przypominam sobie taki fragment jakiegoÅ› tekstu o savoir vivre , chyba nawet w powaÅ¼nej gazecie , choÄ‡ nie moÅ¼na wykluczyÄ‡ , Å¼e zebraÅ‚o mi siÄ™ kiedyÅ› na czytanie kolorowego opakowania do pÅ‚yty DVD , brzmiÄ…cy mniej wiÄ™cej tak \" jeÅ›li Twoi goÅ›cie palÄ… , musisz niestety pozwoliÄ‡ im paliÄ‡ w salonie \" ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "To juÅ¼ mniejsza tolerancja spotyka miÅ‚oÅ›nikÃ³w czosnku na Å›niadanie .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "O pierdzeniu przy stole nie wspominajÄ…c .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Na szczÄ™Å›cie Blogger umoÅ¼liwia dodawanie wpisÃ³w zaplanowanych , czyli moÅ¼na dosÅ‚ownie \" pisaÄ‡ \" o punkt 12 . 00 w Nowy Rok : o ) Ja nie naleÅ¼Ä™ do ludzi ktÃ³rzy wszÄ™dzie biegajÄ… z komputerem albo komÃ³rkÄ… z internetem w Å›rodku , ale wystarczajÄ…co takich znam , wiÄ™c przesyÅ‚am Å¼yczenia w ten sposÃ³b : ) Mam nadziejÄ™ Å¼e bÄ™dÄ™ mogÅ‚a wiÄ™cej blogowaÄ‡ , ale patrzÄ…c na moje Å¼ycie teraz to nie wiem w ogÃ³le co , gdzie i w ktÃ³rÄ…Å¼ stronÄ™ .\n",
      "True tags:\n",
      "['O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "MoÅ¼liwe Å¼e siÄ™ bÄ™dziemy przeprowadzaÄ‡ i w ogÃ³le nic nie wiadomo : p Ale co tam .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Po to wÅ‚aÅ›nie mam mÃ³j blog , aby mieÄ‡ takie spokojne i caÅ‚kiem prywatne i wÅ‚asne miejsce na Å›wiecie .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wszystko jedno gdzie nas los przerzuci . .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "ArtykuÅ‚y archiwalne , publikowane podczas pierwszych prÃ³b tworzenia Obiektywu.net .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O']\n",
      "\n",
      "\n",
      "U progu ubÃ³stwa\n",
      "True tags:\n",
      "['O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O']\n",
      "\n",
      "\n",
      "Konserwatyzm jako postawa Å¼yciowa i ideowa\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "LudzkoÅ›Ä‡ wkroczyÅ‚a w XXI wiek .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PeÅ‚na obaw o przyszÅ‚oÅ›Ä‡ , ale ufna â€¦ nauczona tragicznoÅ›ciÄ… swej historii , ale wciÄ…Å¼ jeszcze nie potrafiÄ…ca jednoznacznie siÄ™ od niej odciÄ…Ä‡ â€¦\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "LudzkoÅ›Ä‡ dÄ…Å¼Ä…ca do rÃ³wnowagi â€¦ ludzkoÅ›Ä‡ , ktÃ³ra choÄ‡ poradziÅ‚a sobie z wiÄ™kszoÅ›ciÄ… reÅ¼imÃ³w politycznych wciÄ…Å¼ jeszcze uwiÄ™ziona w totalitaryzmie wÅ‚asnego zÅ‚a i wÅ‚asnej uÅ‚omnoÅ›ci nie moÅ¼e otwarcie pÄ™dziÄ‡ ku peÅ‚nemu wyzwoleniu â€¦\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "W takich czasach po raz kolejny powraca pytanie o ideÄ™ , ktÃ³ra pozwoliÅ‚a by lepiej ustosunkowaÄ‡ siÄ™ do rzeczywistoÅ›ci , po raz kolejny Å›cierajÄ… siÄ™ postawy i Å›wiatopoglÄ…dy dajÄ…ce ludziom , jeÅ›li nie praktyczne , to choÄ‡by teoretyczne podstawy do okreÅ›lenia ich wÅ‚asnego miejsca na Å›wiecie â€¦\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "CzÅ‚owiek , bÄ™dÄ…c w wiÄ™kszoÅ›ci przypadkÃ³w istotÄ… rozumnÄ… , od zarania dziejÃ³w dÄ…Å¼yÅ‚ do uproszczenia wszelkich czynnoÅ›ci z jakimi borykaÅ‚ siÄ™ podczas swojej egzystencji oraz do zwiÄ™kszenia wydajnoÅ›ci wykonywanej przez siebie pracy .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Ten tok myÅ›lenia przez caÅ‚e tysiÄ…clecia owocowaÅ‚ nowymi wynalazkami , wzrostem wydajnoÅ›ci i opÅ‚acalnoÅ›ci produkcji , oraz skrÃ³ceniem czasu pracy .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Ale to , co wydawaÅ‚o siÄ™ bÅ‚ogosÅ‚awieÅ„stwem dla rozwoju ludzkoÅ›ci , powoli zaczÄ™Å‚o stawaÄ‡ siÄ™ zmorÄ… kaÅ¼dego zatrudnionego w fabryce robotnika i rzemieÅ›lnika .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wynalezienie pierwszego zdolnego do pracy parowego silnika tÅ‚okowego ( W . Brytania , pocz . XVIII w . ) przyspieszyÅ‚o rozwÃ³j budowy maszyn produkcyjnych , a tym samym spowodowaÅ‚o zmniejszenie iloÅ›ci etatÃ³w , wymaganych do wytworzenia produktu finalnego , jak teÅ¼ do prowadzenia i funkcjonowania przedsiÄ™biorstw .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wynalezienie silnika spalinowego i elektrycznego tylko pogÅ‚Ä™biÅ‚o ten proces .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Maszyny wygrywaÅ‚y z ludÅºmi walkÄ™ o zatrudnienie , gdyÅ¼ pracowaÅ‚y bardziej niezawodnie , stale i z wiÄ™kszÄ… precyzjÄ… .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PojawiÅ‚o siÄ™ zatem na ogromnÄ… skalÄ™ zjawisko znane nam dziÅ› jako bezrobocie , a razem z nim niepokoje spoÅ‚eczne ( dajÄ…ce swÃ³j wyraz w niszczeniu maszyn ) i nÄ™dza niÅ¼szych warstw spoÅ‚ecznych .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "PoniewaÅ¼ pierwsza przeglÄ…darka internetowa stworzona przez Bernersa - Lee byÅ‚a jednoczeÅ›nie edytorem stron , przypisuje siÄ™ mu zapoczÄ…tkowanie web 2 . 0 , wiki , blogÃ³w i w ogÃ³le caÅ‚ego UGC .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'B', 'O', 'O', 'B', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O']\n",
      "\n",
      "\n",
      "Wszystko zepsuli wiÄ™c i opÃ³Åºnili Marc Andreessen i Eric Bina , autorzy przeglÄ…darki Mosaic , ktÃ³ra skupiÅ‚a siÄ™ na przeglÄ…daniu , a tworzenie kodu HTML pozostawiÅ‚a innym przyczyniajÄ…c siÄ™ do erozji powagi zawodu informatyka , bo geekiem od HTML mÃ³gÅ‚ zostaÄ‡ kaÅ¼dy .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'B', 'I', 'I', 'I', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'B', 'B', 'I', 'I', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Gdyby uÅ¼ytkownik od poczÄ…tku miaÅ‚ moÅ¼liwoÅ›Ä‡ jednym programem oglÄ…daÄ‡ i tworzyÄ‡ sieÄ‡ , byÄ‡ moÅ¼e nie pojawili by siÄ™ specjaliÅ›ci od siedmiu boleÅ›ci , ktÃ³rzy nie doÅ›Ä‡ , Å¼e uwaÅ¼ajÄ… siÄ™ za wielkich informatykÃ³w , to ich gust ogranicza siÄ™ do niebieskiego tÅ‚a , a w wersji 2 . 0 gradientu .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "MusiaÅ‚o minÄ…Ä‡ kilka lat , Å¼eby znaleÅºli siÄ™ Å›miaÅ‚kowie , ktÃ³rzy przezwyciÄ™Å¼yli fobie i zaczÄ™li uÅ¼ywaÄ‡ przeglÄ…darek nie tylko do przeglÄ…dania .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Poszukiwanie jajka i kury pozostawiam jednak innym , choÄ‡ zgadzam siÄ™ , Å¼e Berners - Lee na pewno wyprzedziÅ‚ swojÄ… epokÄ™ , a jego pomysÅ‚ wciÄ…Å¼ moÅ¼e siÄ™ okazaÄ‡ bardziej przeÅ‚omowy , niÅ¼ nam siÄ™ teraz wydaje .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Z rÃ³Å¼nych notek o blogowaniu przy okazji tych urodzinowych i terminologicznych problemÃ³w zwrÃ³ciÅ‚ mojÄ… uwagÄ™ wpis Schwartza , ktÃ³ry jest CEO Suna i podÄ…Å¼a z duchem czasu i osiÄ…gniÄ™ciami .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Prowadzi dziwny dziennik marketingowo - technologiczny , ktÃ³ry jest ciekawy choÄ‡by z takiej przyczyny , Å¼e nie wiadomo , czy pan Schwartz wyciÄ…gnie Suna z tarapatÃ³w ( na razie na to siÄ™ zanosi ) dziÄ™ki rÃ³Å¼nym modelom biznesowym zwiÄ…zanym z wolnym oprogramowaniem , czy wrÄ™cz przeciwnie ( wciÄ…Å¼ jest to doÅ›Ä‡ prawdopodobne ) .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'O', 'O', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'O', 'O', 'B', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "OtÃ³Å¼ Schwartz zostaÅ‚ poproszony o skomentowanie przypadku innego CEO , ktÃ³ry komentowaÅ‚ pod pseudonimem .\n",
      "True tags:\n",
      "['O', 'B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'B', 'B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Schwartz zostaÅ‚ zakwalifikowany do kategorii blogujÄ…cych CEO , chociaÅ¼ nie chciaÅ‚ .\n",
      "True tags:\n",
      "['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['B', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Po co wiÄ™c pisze tego swojego bloga ( jak gÅ‚osi oficjalna nazwa ) ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Czy kaÅ¼dy , kto coÅ› regularnie pisze w internecie w tym samym miejscu bÄ™dzie juÅ¼ blogerem ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Czy osoba , ktÃ³ra zawsze jako jedna z pierwszych sadzi obszerny komentarz na popularnej stronie bloguje , bo w sumie regularnie i w tym samym miejscu moÅ¼na jÄ… znaleÅºÄ‡ ?\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Schwartz w ogÃ³le chciaÅ‚ by usunÄ…Ä‡ termin \" blogowanie \" , a jeszcze niedawno chciaÅ‚ byÄ‡ \" kropkÄ… w web 2 . 0 \" .\n",
      "True tags:\n",
      "['B', 'I', 'I', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'I', 'I', 'I', 'I', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B', 'B', 'I', 'I', 'I', 'O', 'O']\n",
      "\n",
      "\n",
      "Ale jak przystaÅ‚o na czÅ‚owieka z kucykiem i w krawacie , blogujÄ…cy CEO jest peÅ‚en sprzecznoÅ›ci .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Wiosna wietrzna , ale w koÅ„cu biednemu wiatr w oczy .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "Biednemu , bo przecieÅ¼ tylko taki , co go nie staÄ‡ na samochÃ³d , musi siÄ™ zadowoliÄ‡ poÅ‚owÄ… kÃ³Å‚ek i mÄ™czy siÄ™ na rowerze .\n",
      "True tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n",
      "WrocÅ‚aw jest miastem pozornie pÅ‚askim , ale wyjÄ…tkowo czÄ™sto mam wraÅ¼enie , Å¼e lepiej byÅ‚o by zainwestowaÄ‡ w rower gÃ³rski .\n",
      "True tags:\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "Predicted tags:\n",
      "['B', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [17]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m     last_label \u001B[38;5;241m=\u001B[39m label\n\u001B[0;32m     16\u001B[0m x \u001B[38;5;241m=\u001B[39m tokenizer\u001B[38;5;241m.\u001B[39mencode_plus(tokens, is_split_into_words\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpt\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m---> 17\u001B[0m pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     19\u001B[0m pred_softmaxed \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39msoftmax(pred\u001B[38;5;241m.\u001B[39mlogits[\u001B[38;5;241m0\u001B[39m], dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# first let's fix 'I' token predicted before 'B' token\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1725\u001B[0m, in \u001B[0;36mBertForTokenClassification.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m   1718\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1719\u001B[0m \u001B[38;5;124;03mlabels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\u001B[39;00m\n\u001B[0;32m   1720\u001B[0m \u001B[38;5;124;03m    Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\u001B[39;00m\n\u001B[0;32m   1721\u001B[0m \u001B[38;5;124;03m    1]``.\u001B[39;00m\n\u001B[0;32m   1722\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1723\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m-> 1725\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1726\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1727\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1728\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1729\u001B[0m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1730\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1731\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1732\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1733\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1734\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1735\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1737\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1739\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(sequence_output)\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:995\u001B[0m, in \u001B[0;36mBertModel.forward\u001B[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    986\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m    988\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    989\u001B[0m     input_ids\u001B[38;5;241m=\u001B[39minput_ids,\n\u001B[0;32m    990\u001B[0m     position_ids\u001B[38;5;241m=\u001B[39mposition_ids,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    993\u001B[0m     past_key_values_length\u001B[38;5;241m=\u001B[39mpast_key_values_length,\n\u001B[0;32m    994\u001B[0m )\n\u001B[1;32m--> 995\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    996\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    997\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    998\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    999\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1000\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1001\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1002\u001B[0m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1003\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1004\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1005\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1006\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1007\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m   1008\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:582\u001B[0m, in \u001B[0;36mBertEncoder.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    573\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[0;32m    574\u001B[0m         create_custom_forward(layer_module),\n\u001B[0;32m    575\u001B[0m         hidden_states,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    579\u001B[0m         encoder_attention_mask,\n\u001B[0;32m    580\u001B[0m     )\n\u001B[0;32m    581\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 582\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    584\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    585\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    586\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    587\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    588\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpast_key_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    589\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    590\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    592\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    593\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_cache:\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:510\u001B[0m, in \u001B[0;36mBertLayer.forward\u001B[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001B[0m\n\u001B[0;32m    507\u001B[0m     cross_attn_present_key_value \u001B[38;5;241m=\u001B[39m cross_attention_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m    508\u001B[0m     present_key_value \u001B[38;5;241m=\u001B[39m present_key_value \u001B[38;5;241m+\u001B[39m cross_attn_present_key_value\n\u001B[1;32m--> 510\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    511\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[0;32m    512\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    513\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[0;32m    515\u001B[0m \u001B[38;5;66;03m# if decoder, return the attn key/values as the last output\u001B[39;00m\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\transformers\\modeling_utils.py:2330\u001B[0m, in \u001B[0;36mapply_chunking_to_forward\u001B[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[0m\n\u001B[0;32m   2327\u001B[0m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[0;32m   2328\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcat(output_chunks, dim\u001B[38;5;241m=\u001B[39mchunk_dim)\n\u001B[1;32m-> 2330\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:523\u001B[0m, in \u001B[0;36mBertLayer.feed_forward_chunk\u001B[1;34m(self, attention_output)\u001B[0m\n\u001B[0;32m    521\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[0;32m    522\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate(attention_output)\n\u001B[1;32m--> 523\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mintermediate_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    524\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:438\u001B[0m, in \u001B[0;36mBertOutput.forward\u001B[1;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[0;32m    437\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states, input_tensor):\n\u001B[1;32m--> 438\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    439\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(hidden_states)\n\u001B[0;32m    440\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(hidden_states \u001B[38;5;241m+\u001B[39m input_tensor)\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\torch\\nn\\modules\\linear.py:103\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 103\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\conda-envs\\hf-download-test-finetune\\lib\\site-packages\\torch\\nn\\functional.py:1848\u001B[0m, in \u001B[0;36mlinear\u001B[1;34m(input, weight, bias)\u001B[0m\n\u001B[0;32m   1846\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_variadic(\u001B[38;5;28minput\u001B[39m, weight, bias):\n\u001B[0;32m   1847\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(linear, (\u001B[38;5;28minput\u001B[39m, weight, bias), \u001B[38;5;28minput\u001B[39m, weight, bias\u001B[38;5;241m=\u001B[39mbias)\n\u001B[1;32m-> 1848\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_C\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_nn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "itos = {0: 'O', 1: 'B', 2: 'I', -100: '-'}\n",
    "counter = 5\n",
    "for example in test_set:\n",
    "    tokens = example['tokens']\n",
    "    labels = example['labels']\n",
    "    true_tags = []\n",
    "    last_label = 0\n",
    "    for label in labels[1:-1]:\n",
    "        if label == -100:\n",
    "            if last_label == 1:\n",
    "                label = 2\n",
    "            else:\n",
    "                label = last_label\n",
    "        true_tags.append(itos[label])\n",
    "        last_label = label\n",
    "    x = tokenizer.encode_plus(tokens, is_split_into_words=True, return_tensors='pt')\n",
    "    pred = model(**x)\n",
    "\n",
    "    pred_softmaxed = torch.nn.functional.softmax(pred.logits[0], dim=-1)\n",
    "    # first let's fix 'I' token predicted before 'B' token\n",
    "    predicted_labels = []\n",
    "    last_label = 0\n",
    "    for i in range(pred_softmaxed.shape[0]):\n",
    "        token_preds = pred_softmaxed[i]\n",
    "        if token_preds.argmax(dim=-1) == 2 and last_label == 0:\n",
    "            token_preds[1] = float('-inf')\n",
    "        new_label = token_preds.argmax(dim=-1).item()\n",
    "        last_label = new_label\n",
    "        predicted_labels.append(itos[new_label])\n",
    "    if counter > 0 or predicted_labels != true_tags:\n",
    "        print(' '.join(tokens))\n",
    "        print(f\"True tags:\\n{true_tags}\\n\")\n",
    "        print(f\"Predicted tags:\\n{predicted_labels[1:-1]}\\n\\n\")\n",
    "        counter -= 1\n",
    "    else:\n",
    "        break\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "{'input_ids': tensor([[    0, 15938,  5637,  2921,  2099,  1026,  1335,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['W', 'koÅ„cu', 'wyszÅ‚o', ',', 'do', 'czego', 'potrzebne', 'byÅ‚o', 'Google', 'Gears', '.']\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[    0,  1049,  4988, 15283,  1947,  2041,  2784,  7318,  2404, 22532,\n          4281, 41959,  1899,     2]])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_set = tokenized_kwpr['test']\n",
    "print(example['tokens'])\n",
    "tokenizer.encode(example['tokens'], is_split_into_words=True, return_tensors='pt')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}